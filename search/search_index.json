{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Funcparserlib","text":"<p>Recursive descent parsing library for Python based on functional combinators.</p> <p> </p>"},{"location":"#description","title":"Description","text":"<p>The primary focus of <code>funcparserlib</code> is parsing little languages or external DSLs (domain specific languages).</p> <p>Parsers made with <code>funcparserlib</code> are pure-Python LL(*) parsers. It means that it's very easy to write parsers without thinking about lookaheads and other hardcore parsing stuff. However, recursive descent parsing is a rather slow method compared to LL(k) or LR(k) algorithms. Still, parsing with <code>funcparserlib</code> is at least twice faster than PyParsing, a very popular library for Python.</p> <p>The source code of <code>funcparserlib</code> is only 1.2K lines of code, with lots of comments. Its API is fully type hinted. It features the longest parsed prefix error reporting, as well as a tiny lexer generator for token position tracking.</p> <p>The idea of parser combinators used in <code>funcparserlib</code> comes from the Introduction to Functional Programming course. We have converted it from ML into Python.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install <code>funcparserlib</code> from PyPI:</p> <pre><code>$ pip install funcparserlib\n</code></pre> <p>There are no dependencies on other libraries.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started<ul> <li>Your starting point with <code>funcparserlib</code></li> </ul> </li> <li>API Reference<ul> <li>Learn the details of the API</li> </ul> </li> </ul> <p>There are several examples available in the <code>tests/</code> directory:</p> <ul> <li>GraphViz DOT parser</li> <li>JSON parser</li> </ul> <p>See also the changelog.</p>"},{"location":"#example","title":"Example","text":"<p>Let's consider a little language of numeric expressions with a syntax similar to Python expressions. Here are some expression strings in this language:</p> <pre><code>0\n1 + 2 + 3\n-1 + 2 ** 32\n3.1415926 * (2 + 7.18281828e-1) * 42\n</code></pre> <p>Here is the complete source code of the tokenizer and the parser for this language written using <code>funcparserlib</code>:</p> <pre><code>from typing import List, Tuple, Union\nfrom dataclasses import dataclass\n\nfrom funcparserlib.lexer import make_tokenizer, TokenSpec, Token\nfrom funcparserlib.parser import tok, Parser, many, forward_decl, finished\n\n\n@dataclass\nclass BinaryExpr:\n    op: str\n    left: \"Expr\"\n    right: \"Expr\"\n\n\nExpr = Union[BinaryExpr, int, float]\n\n\ndef tokenize(s: str) -&gt; List[Token]:\n    specs = [\n        TokenSpec(\"whitespace\", r\"\\s+\"),\n        TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n        TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n        TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n    ]\n    tokenizer = make_tokenizer(specs)\n    return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n\n\ndef parse(tokens: List[Token]) -&gt; Expr:\n    int_num = tok(\"int\") &gt;&gt; int\n    float_num = tok(\"float\") &gt;&gt; float\n    number = int_num | float_num\n\n    expr: Parser[Token, Expr] = forward_decl()\n    parenthesized = -op(\"(\") + expr + -op(\")\")\n    primary = number | parenthesized\n    power = primary + many(op(\"**\") + primary) &gt;&gt; to_expr\n    term = power + many((op(\"*\") | op(\"/\")) + power) &gt;&gt; to_expr\n    sum = term + many((op(\"+\") | op(\"-\")) + term) &gt;&gt; to_expr\n    expr.define(sum)\n\n    document = expr + -finished\n\n    return document.parse(tokens)\n\n\ndef op(name: str) -&gt; Parser[Token, str]:\n    return tok(\"op\", name)\n\n\ndef to_expr(args: Tuple[Expr, List[Tuple[str, Expr]]]) -&gt; Expr:\n    first, rest = args\n    result = first\n    for op, expr in rest:\n        result = BinaryExpr(op, result, expr)\n    return result\n</code></pre> <p>Now, consider this numeric expression: <code>3.1415926 * (2 + 7.18281828e-1) * 42</code>.</p> <p>Let's <code>tokenize()</code> it using the tokenizer we've created with <code>funcparserlib.lexer</code>:</p> <pre><code>[\n    Token('float', '3.1415926'),\n    Token('op', '*'),\n    Token('op', '('),\n    Token('int', '2'),\n    Token('op', '+'),\n    Token('float', '7.18281828e-1'),\n    Token('op', ')'),\n    Token('op', '*'),\n    Token('int', '42'),\n]\n</code></pre> <p>Let's <code>parse()</code> these tokens into an expression tree using our parser created with <code>funcparserlib.parser</code>:</p> <pre><code>BinaryExpr(\n    op='*',\n    left=BinaryExpr(\n        op='*',\n        left=3.1415926,\n        right=BinaryExpr(op='+', left=2, right=0.718281828),\n    ),\n    right=42,\n)\n</code></pre> <p>Learn how to write this parser using <code>funcparserlib</code> in the Getting Started guide!</p>"},{"location":"#used-by","title":"Used By","text":"<p>Some open-source projects that use <code>funcparserlib</code> as an explicit dependency:</p> <ul> <li>Hy, a Lisp dialect that's embedded in Python</li> <li>4.7K stars, version <code>~=1.0</code>, Python 3.8+</li> <li>Splash, a JavaScript rendering service with HTTP API, by Scrapinghub</li> <li>3.9K stars, version <code>*</code>. Python 3 in Docker</li> <li>graphite-beacon, a simple alerting system for Graphite metrics</li> <li>453 stars, version <code>==0.3.6</code>, Python 2 and 3</li> <li>blockdiag, generates block-diagram image file from spec-text file</li> <li>194 stars, version <code>&gt;= 1.0.0a0</code>, Python 3.7+</li> <li>kll, Keyboard Layout Language (KLL) compiler</li> <li>113 stars, copied source code, Python 3.5+</li> </ul>"},{"location":"#next","title":"Next","text":"<p>Read the Getting Started guide to start learning <code>funcparserlib</code>.</p>"},{"location":"changes/","title":"The Changelog","text":""},{"location":"changes/#200-to-be-released","title":"2.0.0 \u2014 to be released","text":"<p>Dropped support for Python 2.7 (end of life). For compatibility with Python 2.7 please use version <code>&gt;=1.0,==1.*</code> (<code>~=1.0</code>).</p>"},{"location":"changes/#added","title":"Added","text":"<ul> <li>Added support for Python 3.12</li> </ul>"},{"location":"changes/#changed","title":"Changed","text":"<ul> <li>Dropped support for Python 2.7</li> <li>Dropped support for Python 3.7</li> </ul>"},{"location":"changes/#101-2022-11-04","title":"1.0.1 \u2014 2022-11-04","text":""},{"location":"changes/#added_1","title":"Added","text":"<ul> <li>Added support for Python 3.11</li> </ul>"},{"location":"changes/#100-2022-05-02","title":"1.0.0 \u2014 2022-05-02","text":"<p>The stable 1.0.0 release freezes the API of funcparserlib 0.3.6 which was released on 2013-05-02, with a few bug fixes and small features.</p>"},{"location":"changes/#added_2","title":"Added","text":"<ul> <li>Added support for Python 3.10</li> <li>Added support for Python 3.9   (#63)   (Thanks to @pkulev)</li> <li>Added support for Python 3.8</li> <li>Added <code>-p</code> (the same as <code>skip(p)</code>) with more strict type hints for <code>-p</code> and <code>p1 + p2</code></li> <li>Added <code>tok(type[, value])</code> for more compact grammars, better error messages</li> <li>Added <code>TokenSpec(type, pattern[, flags])</code> to simplify the use of <code>make_tokenizer()</code></li> <li>Added type hints for the public API</li> <li>Added the new library homepage with the new Getting Started guide and the new API   reference</li> </ul>"},{"location":"changes/#changed_1","title":"Changed","text":"<ul> <li>Parse exceptions now show expected tokens and grammar rules at the stopped position   (#52)</li> <li>Dropped support for Python 3.4, 3.5, 3.6 (end of life)</li> <li>Dropped support for Python 2.5, 2.6, 3.3 (end of life), modernized code for Python    3 to run without obsolete <code>2to3</code>   (#57)   (Thanks to @jdufresne)</li> <li>Removed documentation and unit tests from the distribution</li> <li>Switched from setuptools to Poetry</li> <li>Switched to poetry-core for lighter PEP 517 builds   (#73)   (Thanks to @fabaff)</li> <li>Run unit tests on GitHub Actions for all supported Pythons</li> </ul>"},{"location":"changes/#fixed","title":"Fixed","text":"<ul> <li>Fixed <code>TypeError</code> in <code>oneplus</code> when applying it <code>parser + parser</code>    (#66)   (Thanks to @martica)</li> <li>Fixed <code>AttributeError</code> when comparing <code>Token</code> objects to <code>None</code>   (#58)   (Thanks to @Halolegend94)</li> <li>Fixed doctests in the tutorial   (#49)</li> <li>Fixed several cases of wrong expected tokens in error messages</li> </ul>"},{"location":"changes/#036-2013-05-02","title":"0.3.6 \u2014 2013-05-02","text":""},{"location":"changes/#changed_2","title":"Changed","text":"<ul> <li>Python 3 compatibility</li> <li>More info available in exception objects   (#14)</li> </ul>"},{"location":"changes/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed <code>many()</code> that consumed too many tokens in some cases   (#31)</li> </ul>"},{"location":"changes/#035-2011-01-13","title":"0.3.5 \u2014 2011-01-13","text":""},{"location":"changes/#changed_3","title":"Changed","text":"<ul> <li>Python 2.4 compatibility</li> <li>More readable terminal names for error reporting</li> </ul>"},{"location":"changes/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed wrong token positions in lexer error messages</li> </ul>"},{"location":"changes/#034-2009-10-06","title":"0.3.4 \u2014 2009-10-06","text":""},{"location":"changes/#changed_4","title":"Changed","text":"<ul> <li>Switched from <code>setuptools</code> to <code>distutils</code></li> <li>Improved the <code>run-tests</code> utility</li> </ul>"},{"location":"changes/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed importing all symbols from <code>funcparserlib.lexer</code></li> </ul>"},{"location":"changes/#033-2009-08-03","title":"0.3.3 \u2014 2009-08-03","text":""},{"location":"changes/#added_3","title":"Added","text":"<ul> <li>Added a FAQ question about infinite loops in parsers</li> </ul>"},{"location":"changes/#changed_5","title":"Changed","text":"<ul> <li>Debug rule tracing can be enabled again</li> </ul>"},{"location":"changes/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed a bug in results of skip + skip parsers</li> </ul>"},{"location":"changes/#032-2009-07-26","title":"0.3.2 \u2014 2009-07-26","text":""},{"location":"changes/#added_4","title":"Added","text":"<ul> <li>Added the Parsing Stages Illustrated page</li> </ul>"},{"location":"changes/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed some string and number encoding issues in examples</li> </ul>"},{"location":"changes/#031-2009-07-26","title":"0.3.1 \u2014 2009-07-26","text":"<p>Major optimizations (10x faster than the version 0.3).</p>"},{"location":"changes/#added_5","title":"Added","text":"<ul> <li>Added the <code>forward_decl</code> function, that performs better than <code>with_forward_decls</code></li> <li>Added the <code>pretty_tree</code> function for creating pseudo-graphic trees</li> <li>Added the Nested Brackets Mini-HOWTO</li> <li>Added <code>Makefile</code> and this <code>CHANGES.md</code> file</li> </ul>"},{"location":"changes/#changed_6","title":"Changed","text":"<ul> <li>Use a single immutable input sequence in parsers</li> <li>Call a wrapped parser directly using <code>run</code> (without <code>__call__</code>)</li> <li>The slow <code>logging</code> is enabled only when the <code>debug</code> flag is set</li> </ul>"},{"location":"changes/#03-2009-07-23","title":"0.3 \u2014 2009-07-23","text":""},{"location":"changes/#added_6","title":"Added","text":"<ul> <li>Added <code>pure</code> and <code>bind</code> functions on <code>Parser</code>s making them monads</li> <li>Added the Funcparserlib Tutorial</li> <li>Added a JSON parser as an example</li> </ul>"},{"location":"changes/#changed_7","title":"Changed","text":"<ul> <li>Translated the docs from Russian into English</li> </ul>"},{"location":"changes/#02-2009-07-07","title":"0.2 \u2014 2009-07-07","text":""},{"location":"changes/#added_7","title":"Added","text":"<ul> <li>Added the <code>with_forward_decls</code> combinator for dealing with forward declarations</li> </ul>"},{"location":"changes/#changed_8","title":"Changed","text":"<ul> <li>Switched to the iterative implementation of <code>many</code></li> <li>Un-curried the parser function type in order to simplify things</li> <li>Improvements to the DOT parser</li> </ul>"},{"location":"changes/#01-2009-06-26","title":"0.1 \u2014 2009-06-26","text":"<p>Initial release.</p>"},{"location":"api/","title":"API Reference","text":"<p>Funcparserlib consists of the following modules:</p> <ul> <li><code>funcparserlib.lexer</code> \u2014 Regexp-based tokenizer</li> <li><code>funcparserlib.parser</code> \u2014 Functional parsing combinators</li> <li><code>funcparserlib.util</code> \u2014 Various utilities</li> </ul>"},{"location":"api/lexer/","title":"<code>funcparserlib.lexer</code> \u2014 Regexp-based tokenizer","text":""},{"location":"api/lexer/#funcparserlib.lexer.make_tokenizer","title":"<code>funcparserlib.lexer.make_tokenizer(specs)</code>","text":"<p>Make a function that tokenizes text based on the regexp specs.</p> <p>Type: <code>(Sequence[TokenSpec | Tuple]) -&gt; Callable[[str], Iterable[Token]]</code></p> <p>A token spec is <code>TokenSpec</code> instance.</p> <p>Note</p> <p>For legacy reasons, a token spec may also be a tuple of (type, args), where type sets the value of <code>Token.type</code> for the token, and args are the positional arguments for <code>re.compile()</code>: either just (pattern,) or (pattern, flags).</p> <p>It returns a tokenizer function that takes a string and returns an iterable of <code>Token</code> objects, or raises <code>LexerError</code> if it cannot tokenize the string according to its token specs.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; tokenize = make_tokenizer([\n...     TokenSpec(\"space\", r\"\\s+\"),\n...     TokenSpec(\"id\", r\"\\w+\"),\n...     TokenSpec(\"op\", r\"[,!]\"),\n... ])\n&gt;&gt;&gt; text = \"Hello, World!\"\n&gt;&gt;&gt; [t for t in tokenize(text) if t.type != \"space\"]  # noqa\n[Token('id', 'Hello'), Token('op', ','), Token('id', 'World'), Token('op', '!')]\n&gt;&gt;&gt; text = \"Bye?\"\n&gt;&gt;&gt; list(tokenize(text))\nTraceback (most recent call last):\n    ...\nlexer.LexerError: cannot tokenize data: 1,4: \"Bye?\"\n</code></pre>"},{"location":"api/lexer/#funcparserlib.lexer.TokenSpec","title":"<code>funcparserlib.lexer.TokenSpec</code>","text":"<p>A token specification for generating a lexer via <code>make_tokenizer()</code>.</p>"},{"location":"api/lexer/#funcparserlib.lexer.TokenSpec.__init__","title":"<code>funcparserlib.lexer.TokenSpec.__init__(type, pattern, flags=0)</code>","text":"<p>Initialize a <code>TokenSpec</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>type</code> <code>str</code> <p>User-defined type of the token (e.g. <code>\"name\"</code>, <code>\"number\"</code>, <code>\"operator\"</code>)</p> required <code>pattern</code> <code>str</code> <p>Regexp for matching this token type</p> required <code>flags</code> <code>int</code> <p>Regexp flags, the second argument of <code>re.compile()</code></p> <code>0</code>"},{"location":"api/lexer/#funcparserlib.lexer.Token","title":"<code>funcparserlib.lexer.Token</code>","text":"<p>A token object that represents a substring of certain type in your text.</p> <p>You can compare tokens for equality using the <code>==</code> operator. Tokens also define custom <code>repr()</code> and <code>str()</code>.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>User-defined type of the token (e.g. <code>\"name\"</code>, <code>\"number\"</code>, <code>\"operator\"</code>)</p> <code>value</code> <code>str</code> <p>Text value of the token</p> <code>start</code> <code>Optional[Tuple[int, int]]</code> <p>Start position (line, column)</p> <code>end</code> <code>Optional[Tuple[int, int]]</code> <p>End position (line, column)</p>"},{"location":"api/parser/","title":"<code>funcparserlib.parser</code> \u2014 Functional parsing combinators","text":""},{"location":"api/parser/#funcparserlib.parser","title":"<code>funcparserlib.parser</code>","text":"<p>Functional parsing combinators.</p> <p>Parsing combinators define an internal domain-specific language (DSL) for describing the parsing rules of a grammar. The DSL allows you to start with a few primitive parsers, then combine your parsers to get more complex ones, and finally cover the whole grammar you want to parse.</p> <p>The structure of the language:</p> <ul> <li>Class <code>Parser</code><ul> <li>All the primitives and combinators of the language return <code>Parser</code> objects</li> <li>It defines the main <code>Parser.parse(tokens)</code> method</li> </ul> </li> <li>Primitive parsers<ul> <li><code>tok(type, value)</code>, <code>a(value)</code>, <code>some(pred)</code>, <code>forward_decl()</code>, <code>finished</code></li> </ul> </li> <li>Parser combinators<ul> <li><code>p1 + p2</code>, <code>p1 | p2</code>, <code>p &gt;&gt; f</code>, <code>-p</code>, <code>maybe(p)</code>, <code>many(p)</code>, <code>oneplus(p)</code>,   <code>skip(p)</code></li> </ul> </li> <li>Abstraction<ul> <li>Use regular Python variables <code>p = ...  # Expression of type Parser</code> to define new   rules (non-terminals) of your grammar</li> </ul> </li> </ul> <p>Every time you apply one of the combinators, you get a new <code>Parser</code> object. In other words, the set of <code>Parser</code> objects is closed under the means of combination.</p> <p>Note</p> <p>We took the parsing combinators language from the book Introduction to Functional Programming and translated it from ML into Python.</p>"},{"location":"api/parser/#funcparserlib.parser.Parser","title":"<code>funcparserlib.parser.Parser</code>","text":"<p>             Bases: <code>Generic[_A, _B]</code></p> <p>A parser object that can parse a sequence of tokens or can be combined with other parsers using <code>+</code>, <code>|</code>, <code>&gt;&gt;</code>, <code>many()</code>, and other parsing combinators.</p> <p>Type: <code>Parser[A, B]</code></p> <p>The generic variables in the type are: <code>A</code> \u2014 the type of the tokens in the sequence to parse,<code>B</code> \u2014 the type of the parsed value.</p> <p>In order to define a parser for your grammar:</p> <ol> <li>You start with primitive parsers by calling <code>a(value)</code>, <code>some(pred)</code>,    <code>forward_decl()</code>, <code>finished</code></li> <li>You use parsing combinators <code>p1 + p2</code>, <code>p1 | p2</code>, <code>p &gt;&gt; f</code>, <code>many(p)</code>, and    others to combine parsers into a more complex parser</li> <li>You can assign complex parsers to variables to define names that correspond to    the rules of your grammar</li> </ol> <p>Note</p> <p>The constructor <code>Parser.__init__()</code> is considered internal and may be changed in future versions. Use primitive parsers and parsing combinators to construct new parsers.</p>"},{"location":"api/parser/#funcparserlib.parser.Parser.parse","title":"<code>funcparserlib.parser.Parser.parse(tokens)</code>","text":"<p>Parse the sequence of tokens and return the parsed value.</p> <p>Type: <code>(Sequence[A]) -&gt; B</code></p> <p>It takes a sequence of tokens of arbitrary type <code>A</code> and returns the parsed value of arbitrary type <code>B</code>.</p> <p>If the parser fails to parse the tokens, it raises <code>NoParseError</code>.</p> <p>Note</p> <p>Although <code>Parser.parse()</code> can parse sequences of any objects (including <code>str</code> which is a sequence of <code>str</code> chars), the recommended way is parsing sequences of <code>Token</code> objects.</p> <p>You should use a regexp-based tokenizer <code>make_tokenizer()</code> defined in <code>funcparserlib.lexer</code> to convert your text into a sequence of <code>Token</code> objects before parsing it. You will get more readable parsing error messages (as <code>Token</code> objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.</p>"},{"location":"api/parser/#funcparserlib.parser.Parser.define","title":"<code>funcparserlib.parser.Parser.define(p)</code>","text":"<p>Define the parser created earlier as a forward declaration.</p> <p>Type: <code>(Parser[A, B]) -&gt; None</code></p> <p>Use <code>p = forward_decl()</code> in combination with <code>p.define(...)</code> to define recursive parsers.</p> <p>See the examples in the docs for <code>forward_decl()</code>.</p>"},{"location":"api/parser/#funcparserlib.parser.Parser.named","title":"<code>funcparserlib.parser.Parser.named(name)</code>","text":"<p>Specify the name of the parser for easier debugging.</p> <p>Type: <code>(str) -&gt; Parser[A, B]</code></p> <p>This name is used in the debug-level parsing log. You can also get it via the <code>Parser.name</code> attribute.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = (a(\"x\") + a(\"y\")).named(\"expr\")\n&gt;&gt;&gt; expr.name\n'expr'\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + a(\"y\")\n&gt;&gt;&gt; expr.name\n\"('x', 'y')\"\n</code></pre> <p>Note</p> <p>You can enable the parsing log this way:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\nimport funcparserlib.parser\nfuncparserlib.parser.debug = True\n</code></pre> <p>The way to enable the parsing log may be changed in future versions.</p>"},{"location":"api/parser/#primitive-parsers","title":"Primitive Parsers","text":""},{"location":"api/parser/#funcparserlib.parser.tok","title":"<code>funcparserlib.parser.tok(type, value=None)</code>","text":"<p>Return a parser that parses a <code>Token</code> and returns the string value of the token.</p> <p>Type: <code>(str, Optional[str]) -&gt; Parser[Token, str]</code></p> <p>You can match any token of the specified <code>type</code> or you can match a specific token by its <code>type</code> and <code>value</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = tok(\"expr\")\n&gt;&gt;&gt; expr.parse([Token(\"expr\", \"foo\")])\n'foo'\n&gt;&gt;&gt; expr.parse([Token(\"expr\", \"bar\")])\n'bar'\n&gt;&gt;&gt; expr.parse([Token(\"op\", \"=\")])\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: '=', expected: expr\n</code></pre> <pre><code>&gt;&gt;&gt; expr = tok(\"op\", \"=\")\n&gt;&gt;&gt; expr.parse([Token(\"op\", \"=\")])\n'='\n&gt;&gt;&gt; expr.parse([Token(\"op\", \"+\")])\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: '+', expected: '='\n</code></pre> <p>Note</p> <p>In order to convert your text to parse into a sequence of <code>Token</code> objects, use a regexp-based tokenizer <code>make_tokenizer()</code> defined in <code>funcparserlib.lexer</code>. You will get more readable parsing error messages (as <code>Token</code> objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.</p>"},{"location":"api/parser/#funcparserlib.parser.a","title":"<code>funcparserlib.parser.a(value)</code>","text":"<p>Return a parser that parses a token if it's equal to <code>value</code>.</p> <p>Type: <code>(A) -&gt; Parser[A, A]</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = a(\"x\")\n&gt;&gt;&gt; expr.parse(\"x\")\n'x'\n&gt;&gt;&gt; expr.parse(\"y\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'y', expected: 'x'\n</code></pre> <p>Note</p> <p>Although <code>Parser.parse()</code> can parse sequences of any objects (including <code>str</code> which is a sequence of <code>str</code> chars), the recommended way is parsing sequences of <code>Token</code> objects.</p> <p>You should use a regexp-based tokenizer <code>make_tokenizer()</code> defined in <code>funcparserlib.lexer</code> to convert your text into a sequence of <code>Token</code> objects before parsing it. You will get more readable parsing error messages (as <code>Token</code> objects contain their position in the source file) and good separation of the lexical and syntactic levels of the grammar.</p>"},{"location":"api/parser/#funcparserlib.parser.some","title":"<code>funcparserlib.parser.some(pred)</code>","text":"<p>Return a parser that parses a token if it satisfies the predicate <code>pred</code>.</p> <p>Type: <code>(Callable[[A], bool]) -&gt; Parser[A, A]</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = some(lambda s: s.isalpha()).named('alpha')\n&gt;&gt;&gt; expr.parse(\"x\")\n'x'\n&gt;&gt;&gt; expr.parse(\"y\")\n'y'\n&gt;&gt;&gt; expr.parse(\"1\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: '1', expected: alpha\n</code></pre> <p>Warning</p> <p>The <code>some()</code> combinator is quite slow and may be changed or removed in future versions. If you need a parser for a token by its type (e.g. any identifier) and maybe its value, use <code>tok(type[, value])</code> instead. You should use <code>make_tokenizer()</code> from <code>funcparserlib.lexer</code> to tokenize your text first.</p>"},{"location":"api/parser/#funcparserlib.parser.forward_decl","title":"<code>funcparserlib.parser.forward_decl()</code>","text":"<p>Return an undefined parser that can be used as a forward declaration.</p> <p>Type: <code>Parser[Any, Any]</code></p> <p>Use <code>p = forward_decl()</code> in combination with <code>p.define(...)</code> to define recursive parsers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; expr.define(a(\"x\") + maybe(expr) + a(\"y\"))\n&gt;&gt;&gt; expr.parse(\"xxyy\")  # noqa\n('x', ('x', None, 'y'), 'y')\n&gt;&gt;&gt; expr.parse(\"xxy\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected end of input, expected: 'y'\n</code></pre> <p>Note</p> <p>If you care about static types, you should add a type hint for your forward declaration, so that your type checker can check types in <code>p.define(...)</code> later:</p> <pre><code>p: Parser[str, int] = forward_decl()\np.define(a(\"x\"))  # Type checker error\np.define(a(\"1\") &gt;&gt; int)  # OK\n</code></pre>"},{"location":"api/parser/#finished","title":"<code>finished</code>","text":"<p>A parser that throws an exception if there are any unparsed tokens left in the sequence.</p> <p>Type: <code>Parser[Any, None]</code></p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import a, finished\n&gt;&gt;&gt; expr = a(\"x\") + finished\n&gt;&gt;&gt; expr.parse(\"x\")\n('x', None)\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + finished\n&gt;&gt;&gt; expr.parse(\"xy\")\nTraceback (most recent call last):\n    ...\nfuncparserlib.parser.NoParseError: got unexpected token: 'y', expected: end of input\n</code></pre>"},{"location":"api/parser/#parser-combinators","title":"Parser Combinators","text":""},{"location":"api/parser/#funcparserlib.parser.Parser.__add__","title":"<code>funcparserlib.parser.Parser.__add__(other)</code>","text":"<p>Sequential combination of parsers. It runs this parser, then the other parser.</p> <p>The return value of the resulting parser is a tuple of each parsed value in the sum of parsers. We merge all parsing results of <code>p1 + p2 + ... + pN</code> into a single tuple. It means that the parsing result may be a 2-tuple, a 3-tuple, a 4-tuple, etc. of parsed values. You avoid this by transforming the parsed pair into a new value using the <code>&gt;&gt;</code> combinator.</p> <p>You can also skip some parsing results in the resulting parsers by using <code>-p</code> or <code>skip(p)</code> for some parsers in your sum of parsers. It means that the parsing result might be a single value, not a tuple of parsed values. See the docs for <code>Parser.__neg__()</code> for more examples.</p> <p>Overloaded types (lots of them to provide stricter checking for the quite dynamic return type of this method):</p> <ul> <li><code>(self: Parser[A, B], _IgnoredParser[A]) -&gt; Parser[A, B]</code></li> <li><code>(self: Parser[A, B], Parser[A, C]) -&gt; _TupleParser[A, Tuple[B, C]]</code></li> <li><code>(self: _TupleParser[A, B], _IgnoredParser[A]) -&gt; _TupleParser[A, B]</code></li> <li><code>(self: _TupleParser[A, B], Parser[A, Any]) -&gt; Parser[A, Any]</code></li> <li><code>(self: _IgnoredParser[A], _IgnoredParser[A]) -&gt; _IgnoredParser[A]</code></li> <li><code>(self: _IgnoredParser[A], Parser[A, C]) -&gt; Parser[A, C]</code></li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + a(\"y\")\n&gt;&gt;&gt; expr.parse(\"xy\")\n('x', 'y')\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + a(\"y\") + a(\"z\")\n&gt;&gt;&gt; expr.parse(\"xyz\")\n('x', 'y', 'z')\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + a(\"y\")\n&gt;&gt;&gt; expr.parse(\"xz\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'z', expected: 'y'\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.Parser.__neg__","title":"<code>funcparserlib.parser.Parser.__neg__()</code>","text":"<p>Return a parser that parses the same tokens, but its parsing result is ignored by the sequential <code>+</code> combinator.</p> <p>Type: <code>(Parser[A, B]) -&gt; _IgnoredParser[A]</code></p> <p>You can use it for throwing away elements of concrete syntax (e.g. <code>\",\"</code>, <code>\";\"</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = -a(\"x\") + a(\"y\")\n&gt;&gt;&gt; expr.parse(\"xy\")\n'y'\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + -a(\"y\")\n&gt;&gt;&gt; expr.parse(\"xy\")\n'x'\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + -a(\"y\") + a(\"z\")\n&gt;&gt;&gt; expr.parse(\"xyz\")\n('x', 'z')\n</code></pre> <pre><code>&gt;&gt;&gt; expr = -a(\"x\") + a(\"y\") + -a(\"z\")\n&gt;&gt;&gt; expr.parse(\"xyz\")\n'y'\n</code></pre> <pre><code>&gt;&gt;&gt; expr = -a(\"x\") + a(\"y\")\n&gt;&gt;&gt; expr.parse(\"yz\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'y', expected: 'x'\n</code></pre> <pre><code>&gt;&gt;&gt; expr = a(\"x\") + -a(\"y\")\n&gt;&gt;&gt; expr.parse(\"xz\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'z', expected: 'y'\n</code></pre> <p>Note</p> <p>You should not pass the resulting parser to any combinators other than <code>+</code>. You should have at least one non-skipped value in your <code>p1 + p2 + ... + pN</code>. The parsed value of <code>-p</code> is an internal <code>_Ignored</code> object, not intended for actual use.</p>"},{"location":"api/parser/#funcparserlib.parser.Parser.__or__","title":"<code>funcparserlib.parser.Parser.__or__(other)</code>","text":"<p>Choice combination of parsers.</p> <p>It runs this parser and returns its result. If the parser fails, it runs the other parser.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = a(\"x\") | a(\"y\")\n&gt;&gt;&gt; expr.parse(\"x\")\n'x'\n&gt;&gt;&gt; expr.parse(\"y\")\n'y'\n&gt;&gt;&gt; expr.parse(\"z\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'z', expected: 'x' or 'y'\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.Parser.__rshift__","title":"<code>funcparserlib.parser.Parser.__rshift__(f)</code>","text":"<p>Transform the parsing result by applying the specified function.</p> <p>Type: <code>(Callable[[B], C]) -&gt; Parser[A, C]</code></p> <p>You can use it for transforming the parsed value into another value before including it into the parse tree (the AST).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; def make_canonical_name(s):\n...     return s.lower()\n&gt;&gt;&gt; expr = (a(\"D\") | a(\"d\")) &gt;&gt; make_canonical_name\n&gt;&gt;&gt; expr.parse(\"D\")\n'd'\n&gt;&gt;&gt; expr.parse(\"d\")\n'd'\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.maybe","title":"<code>funcparserlib.parser.maybe(p)</code>","text":"<p>Return a parser that returns <code>None</code> if the parser <code>p</code> fails.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = maybe(a(\"x\"))\n&gt;&gt;&gt; expr.parse(\"x\")\n'x'\n&gt;&gt;&gt; expr.parse(\"y\") is None\nTrue\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.many","title":"<code>funcparserlib.parser.many(p)</code>","text":"<p>Return a parser that applies the parser <code>p</code> as many times as it succeeds at parsing the tokens.</p> <p>Return a parser that infinitely applies the parser <code>p</code> to the input sequence of tokens as long as it successfully parses them. The parsed value is a list of the sequentially parsed values.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = many(a(\"x\"))\n&gt;&gt;&gt; expr.parse(\"x\")\n['x']\n&gt;&gt;&gt; expr.parse(\"xx\")\n['x', 'x']\n&gt;&gt;&gt; expr.parse(\"xxxy\")  # noqa\n['x', 'x', 'x']\n&gt;&gt;&gt; expr.parse(\"y\")\n[]\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.oneplus","title":"<code>funcparserlib.parser.oneplus(p)</code>","text":"<p>Return a parser that applies the parser <code>p</code> one or more times.</p> <p>A similar parser combinator <code>many(p)</code> means apply <code>p</code> zero or more times, whereas <code>oneplus(p)</code> means apply <code>p</code> one or more times.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; expr = oneplus(a(\"x\"))\n&gt;&gt;&gt; expr.parse(\"x\")\n['x']\n&gt;&gt;&gt; expr.parse(\"xx\")\n['x', 'x']\n&gt;&gt;&gt; expr.parse(\"y\")\nTraceback (most recent call last):\n    ...\nparser.NoParseError: got unexpected token: 'y', expected: 'x'\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.skip","title":"<code>funcparserlib.parser.skip(p)</code>","text":"<p>An alias for <code>-p</code>.</p> <p>See also the docs for <code>Parser.__neg__()</code>.</p>"},{"location":"api/parser/#extra-parser-monad","title":"Extra: Parser Monad","text":"<p>As a functional programmer, you might be pleased to know, that parsers in funcparserlib form a monad with <code>Parser.bind()</code> as <code>&gt;&gt;=</code> and <code>pure()</code> as <code>return</code>.</p> <p>We could have expressed other parsing combinators in terms of <code>bind()</code>, but would be inefficient in Python:</p> <pre><code># noinspection PyUnresolvedReferences\nclass Parser:\n    def __add__(self, other):\n        return self.bind(lambda x: other.bind(lambda y: pure((x, y))))\n\n    def __rshift__(self, other):\n        return self.bind(lambda x: pure(x))\n</code></pre>"},{"location":"api/parser/#funcparserlib.parser.Parser.bind","title":"<code>funcparserlib.parser.Parser.bind(f)</code>","text":"<p>Bind the parser to a monadic function that returns a new parser.</p> <p>Type: <code>(Callable[[B], Parser[A, C]]) -&gt; Parser[A, C]</code></p> <p>Also known as <code>&gt;&gt;=</code> in Haskell.</p> <p>Note</p> <p>You can parse any context-free grammar without resorting to <code>bind</code>. Due to its poor performance please use it only when you really need it.</p>"},{"location":"api/parser/#funcparserlib.parser.pure","title":"<code>funcparserlib.parser.pure(x)</code>","text":"<p>Wrap any object into a parser.</p> <p>Type: <code>(A) -&gt; Parser[A, A]</code></p> <p>A pure parser doesn't touch the tokens sequence, it just returns its pure <code>x</code> value.</p> <p>Also known as <code>return</code> in Haskell.</p>"},{"location":"api/util/","title":"<code>funcparserlib.util</code> \u2014 Various utilities","text":""},{"location":"api/util/#funcparserlib.util.pretty_tree","title":"<code>funcparserlib.util.pretty_tree(x, kids, show)</code>","text":"<p>Return a pseudo-graphic tree representation of the object <code>x</code> similar to the <code>tree</code> command in Unix.</p> <p>Type: <code>(T, Callable[[T], List[T]], Callable[[T], str]) -&gt; str</code></p> <p>It applies the parameter <code>show</code> (which is a function of type <code>(T) -&gt; str</code>) to get a textual representation of the objects to show.</p> <p>It applies the parameter <code>kids</code> (which is a function of type <code>(T) -&gt; List[T]</code>) to list the children of the object to show.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; print(pretty_tree(\n...     [\"foo\", [\"bar\", \"baz\"], \"quux\"],\n...     lambda obj: obj if isinstance(obj, list) else [],\n...     lambda obj: \"[]\" if isinstance(obj, list) else str(obj),\n... ))\n[]\n|-- foo\n|-- []\n|   |-- bar\n|   `-- baz\n`-- quux\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#intro","title":"Intro","text":"<p>In this guide, we will write a parser for a numeric expression calculator with a syntax similar to Python expressions. Writing a calculator is a common example in articles related to parsers and parsing techniques, so it is a good starting point in learning <code>funcparserlib</code>.</p> <p>You will learn how to write a parser of numeric expressions using <code>funcparserlib</code>. Here are some expression strings we want to be able to parse:</p> <pre><code>0\n1 + 2 + 3\n-1 + 2 ** 32\n3.1415926 * (2 + 7.18281828e-1) * 42\n</code></pre> <p>We will parse these strings into trees of objects like this one:</p> <pre><code>BinaryExpr('*')\n|-- BinaryExpr('*')\n|   |-- 3.1415926\n|   `-- BinaryExpr('+')\n|       |-- 2\n|       `-- 0.718281828\n`-- 42\n</code></pre>"},{"location":"getting-started/#diving-in","title":"Diving In","text":"<p>Here is the complete source code of the expression parser we are going to write.</p> <p>You are not supposed to understand it now. Just look at its shape and try to get some feeling about its structure. By the end of this guide, you will fully understand this code and will be able to write parsers for your own needs.</p> <pre><code>&gt;&gt;&gt; from typing import List, Tuple, Union\n&gt;&gt;&gt; from dataclasses import dataclass\n&gt;&gt;&gt; from funcparserlib.lexer import make_tokenizer, TokenSpec, Token\n&gt;&gt;&gt; from funcparserlib.parser import tok, Parser, many, forward_decl, finished\n\n\n&gt;&gt;&gt; @dataclass\n... class BinaryExpr:\n...     op: str\n...     left: \"Expr\"\n...     right: \"Expr\"\n\n\n&gt;&gt;&gt; Expr = Union[BinaryExpr, int, float]\n\n\n&gt;&gt;&gt; def tokenize(s: str) -&gt; List[Token]:\n...     specs = [\n...         TokenSpec(\"whitespace\", r\"\\s+\"),\n...         TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n...         TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n...         TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n...     ]\n...     tokenizer = make_tokenizer(specs)\n...     return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n\n\n&gt;&gt;&gt; def parse(tokens: List[Token]) -&gt; Expr:\n...     int_num = tok(\"int\") &gt;&gt; int\n...     float_num = tok(\"float\") &gt;&gt; float\n...     number = int_num | float_num\n...\n...     expr: Parser[Token, Expr] = forward_decl()\n...     parenthesized = -op(\"(\") + expr + -op(\")\")\n...     primary = number | parenthesized\n...     power = primary + many(op(\"**\") + primary) &gt;&gt; to_expr\n...     term = power + many((op(\"*\") | op(\"/\")) + power) &gt;&gt; to_expr\n...     sum = term + many((op(\"+\") | op(\"-\")) + term) &gt;&gt; to_expr\n...     expr.define(sum)\n...\n...     document = expr + -finished\n...\n...     return document.parse(tokens)\n\n\n&gt;&gt;&gt; def op(name: str) -&gt; Parser[Token, str]:\n...     return tok(\"op\", name)\n\n\n&gt;&gt;&gt; def to_expr(args: Tuple[Expr, List[Tuple[str, Expr]]]) -&gt; Expr:\n...     first, rest = args\n...     result = first\n...     for op, expr in rest:\n...         result = BinaryExpr(op, result, expr)\n...     return result\n</code></pre> <p>Note</p> <p>The code examples in this guide are actually executable. You can clone the funcparserlib repository from GitHub and run the examples from the document via <code>doctest</code>:</p> <pre><code>python3 -m doctest -v docs/getting-started/*.md\n</code></pre> <p>Test the expression parser:</p> <pre><code>&gt;&gt;&gt; parse(tokenize(\"0\"))\n0\n\n&gt;&gt;&gt; parse(tokenize(\"1 + 2 + 3\"))\nBinaryExpr(op='+', left=BinaryExpr(op='+', left=1, right=2), right=3)\n\n&gt;&gt;&gt; parse(tokenize(\"-1 + 2 ** 32\"))\nBinaryExpr(op='+', left=-1, right=BinaryExpr(op='**', left=2, right=32))\n\n&gt;&gt;&gt; parse(tokenize(\"3.1415926 * (2 + 7.18281828e-1) * 42\"))\nBinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42)\n</code></pre>"},{"location":"getting-started/#next","title":"Next","text":"<p>Now let's start learning how to write a numeric expression parser using <code>funcparserlib</code>.</p> <p>In the next chapter you will learn about the first step in parsing: tokenizing the input. It means splitting your input string into a sequence of tokens that are easier to parse.</p>"},{"location":"getting-started/parse-tree/","title":"Preparing the Parse Tree","text":"<p>So far we have defined the parser for our calculator expressions language:</p> <pre><code>&gt;&gt;&gt; from typing import List\n&gt;&gt;&gt; from funcparserlib.lexer import make_tokenizer, TokenSpec, Token\n&gt;&gt;&gt; from funcparserlib.parser import tok, Parser, many, forward_decl, finished\n\n\n&gt;&gt;&gt; def tokenize(s: str) -&gt; List[Token]:\n...     specs = [\n...         TokenSpec(\"whitespace\", r\"\\s+\"),\n...         TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n...         TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n...         TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n...     ]\n...     tokenizer = make_tokenizer(specs)\n...     return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n\n\n&gt;&gt;&gt; def op(name: str) -&gt; Parser[Token, str]:\n...     return tok(\"op\", name)\n\n\n&gt;&gt;&gt; int_str = tok(\"int\")\n&gt;&gt;&gt; float_str = tok(\"float\")\n&gt;&gt;&gt; number = int_str | float_str\n&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; parenthesized = op(\"(\") + expr + op(\")\")\n&gt;&gt;&gt; primary = number | parenthesized\n&gt;&gt;&gt; power = primary + many(op(\"**\") + primary)\n&gt;&gt;&gt; expr.define(power)\n&gt;&gt;&gt; document = expr + finished\n</code></pre> <p>Here is how its parse results look so far:</p> <pre><code>&gt;&gt;&gt; document.parse(tokenize(\"2 ** (3 ** 4)\"))\n('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None)\n</code></pre>"},{"location":"getting-started/parse-tree/#p-f-transforming-parse-results","title":"<code>p &gt;&gt; f</code>: Transforming Parse Results","text":"<p>Let's start improving our parse results by converting numbers from <code>str</code> to <code>int</code> or <code>float</code>. We will use the <code>Parser.__rshift__</code> combinator for that. <code>p &gt;&gt; f</code> takes a parser <code>p</code> and a function <code>f</code> of a single argument and returns a new parser, that applies <code>f</code> to the parse result of <code>p</code>.</p> <p>An integer parser that returns <code>int</code> values:</p> <pre><code>&gt;&gt;&gt; int_num: Parser[Token, int] = tok(\"int\") &gt;&gt; int\n</code></pre> <p>Note</p> <p>We specify the type hint for the parser only for clarity here. We wanted to highlight that <code>&gt;&gt;</code> here changes the output type of the parser from <code>str</code> to <code>int</code>. You may omit type hints for parsers and rely on type inference features of your text editor and type checker to get code completion and linting warnings:</p> <pre><code>&gt;&gt;&gt; int_num = tok(\"int\") &gt;&gt; int\n</code></pre> <p>The only combinator which type is not inferrable is <code>forward_decl()</code>. You should specify its type explicitly to get your parser fully type checked.</p> <p>Try it:</p> <pre><code>&gt;&gt;&gt; int_num.parse(tokenize(\"42\"))\n42\n</code></pre> <p>Let's redefine our <code>number</code> parser so that it returns either <code>int</code> or <code>float</code>:</p> <pre><code>&gt;&gt;&gt; from typing import Union\n\n\n&gt;&gt;&gt; float_num: Parser[Token, float] = tok(\"float\") &gt;&gt; float\n&gt;&gt;&gt; number: Parser[Token, Union[int, float]] = int_num | float_num\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; number.parse(tokenize(\"42\"))\n42\n\n&gt;&gt;&gt; number.parse(tokenize(\"3.14\"))\n3.14\n</code></pre>"},{"location":"getting-started/parse-tree/#-p-skipping-parse-results","title":"<code>-p</code>: Skipping Parse Results","text":"<p>Let's recall our nested parenthesized numbers example:</p> <pre><code>&gt;&gt;&gt; p = forward_decl()\n&gt;&gt;&gt; p.define(number | (op(\"(\") + p + op(\")\")))\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; p.parse(tokenize(\"((1))\"))\n('(', ('(', 1, ')'), ')')\n</code></pre> <p>We have successfully parsed numbers in nested parentheses, but we don't want to see parentheses in the parsing results. Let's skip them using the <code>Parser.__neg__</code> combinator. It allows you to skip any parts of a sequence of parsers concatenated via <code>p1 + p2 + ... + pN</code> by using a unary <code>-p</code> operator on the ones you want to skip:</p> <pre><code>&gt;&gt;&gt; p = forward_decl()\n&gt;&gt;&gt; p.define(number | (-op(\"(\") + p + -op(\")\")))\n</code></pre> <p>The result is cleaner now:</p> <pre><code>&gt;&gt;&gt; p.parse(tokenize(\"1\"))\n1\n\n&gt;&gt;&gt; p.parse(tokenize(\"(1)\"))\n1\n\n&gt;&gt;&gt; p.parse(tokenize(\"((1))\"))\n1\n</code></pre> <p>Let's re-define our grammar using the <code>Parser.__neg__</code> combinator to get rid of extra parentheses in the parse results, as well as of extra <code>None</code> returned by <code>finished</code>:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; parenthesized = -op(\"(\") + expr + -op(\")\")\n&gt;&gt;&gt; primary = number | parenthesized\n&gt;&gt;&gt; power = primary + many(op(\"**\") + primary)\n&gt;&gt;&gt; expr.define(power)\n&gt;&gt;&gt; document = expr + -finished\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; document.parse(tokenize(\"2 ** (3 ** 4)\"))\n(2, [('**', (3, [('**', 4)]))])\n</code></pre>"},{"location":"getting-started/parse-tree/#user-defined-classes-for-the-parse-tree","title":"User-Defined Classes for the Parse Tree","text":"<p>We have many types of binary operators in our grammar, but we've defined only the <code>**</code> power operator so far. Let's define them for <code>*</code>, <code>/</code>, <code>+</code>, <code>-</code> as well:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; parenthesized = -op(\"(\") + expr + -op(\")\")\n&gt;&gt;&gt; primary = number | parenthesized\n&gt;&gt;&gt; power = primary + many(op(\"**\") + primary)\n&gt;&gt;&gt; term = power + many((op(\"*\") | op(\"/\")) + power)\n&gt;&gt;&gt; sum = term + many((op(\"+\") | op(\"-\")) + term)\n&gt;&gt;&gt; expr.define(sum)\n&gt;&gt;&gt; document = expr + -finished\n</code></pre> <p>Here we've introduced a hierarchy of nested parsers: <code>expr -&gt; sum -&gt; term -&gt; power -&gt; primary -&gt; parenthesized -&gt; expr -&gt; ...</code> to reflect the order of calculations set by our operator priorities: <code>+</code> &lt; <code>*</code> &lt; <code>**</code> &lt; <code>()</code>.</p> <p>Test it:</p> <pre><code>&gt;&gt;&gt; document.parse(tokenize(\"1 * (2 + 0) ** 3\"))\n(1, [], [('*', (2, [], [], [('+', (0, [], []))], [('**', 3)]))], [])\n</code></pre> <p>It's hard to understand the results without proper user-defined classes for our expression types. We actually have 3 expression types:</p> <ul> <li>Integer numbers</li> <li>Floating point numbers</li> <li>Binary expressions</li> </ul> <p>For integers and floats we will use Python <code>int</code> and <code>float</code> classes. For binary expressions we'll introduce the <code>BinaryExpr</code> class:</p> <pre><code>&gt;&gt;&gt; from dataclasses import dataclass\n\n\n&gt;&gt;&gt; @dataclass\n... class BinaryExpr:\n...     op: str\n...     left: \"Expr\"\n...     right: \"Expr\"\n</code></pre> <p>Since we don't use a common base class for our expressions, we have to define <code>Expr</code> as a union of possible expression types:</p> <pre><code>&gt;&gt;&gt; Expr = Union[BinaryExpr, int, float]\n</code></pre> <p>Now let's define a function to transform the parse results of our binary operators into <code>BinaryExpr</code> objects. Take a look at our parsers of various binary expressions. You can infer that each of them returns (expression, list of (operator, expression)). We will transform these nested tuples and lists into a tree of nested expressions by defining a function <code>to_expr(args)</code> and applying <code>&gt;&gt; to_expr</code> to our expression parsers:</p> <pre><code>&gt;&gt;&gt; from typing import Tuple\n\n\n&gt;&gt;&gt; def to_expr(args: Tuple[Expr, List[Tuple[str, Expr]]]) -&gt; Expr:\n...     first, rest = args\n...     result = first\n...     for op, expr in rest:\n...         result = BinaryExpr(op, result, expr)\n...     return result\n</code></pre> <p>Let's re-define our grammar using this transformation:</p> <pre><code>&gt;&gt;&gt; expr: Parser[Token, Expr] = forward_decl()\n&gt;&gt;&gt; parenthesized = -op(\"(\") + expr + -op(\")\")\n&gt;&gt;&gt; primary = number | parenthesized\n&gt;&gt;&gt; power = primary + many(op(\"**\") + primary) &gt;&gt; to_expr\n&gt;&gt;&gt; term = power + many((op(\"*\") | op(\"/\")) + power) &gt;&gt; to_expr\n&gt;&gt;&gt; sum = term + many((op(\"+\") | op(\"-\")) + term) &gt;&gt; to_expr\n&gt;&gt;&gt; expr.define(sum)\n&gt;&gt;&gt; document = expr + -finished\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; document.parse(tokenize(\"3.1415926 * (2 + 7.18281828e-1) * 42\"))\nBinaryExpr(op='*', left=BinaryExpr(op='*', left=3.1415926, right=BinaryExpr(op='+', left=2, right=0.718281828)), right=42)\n</code></pre> <p>Let's pretty-print it using the <code>pretty_tree(x, kids, show)</code> function:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.util import pretty_tree\n\n\n&gt;&gt;&gt; def pretty_expr(expr: Expr) -&gt; str:\n... \n...     def kids(expr: Expr) -&gt; List[Expr]:\n...         if isinstance(expr, BinaryExpr):\n...             return [expr.left, expr.right]\n...         else:\n...             return []\n... \n...     def show(expr: Expr) -&gt; str:\n...         if isinstance(expr, BinaryExpr):\n...             return f\"BinaryExpr({expr.op!r})\"\n...         else:\n...             return repr(expr)\n... \n...     return pretty_tree(expr, kids, show)\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; print(pretty_expr(document.parse(tokenize(\"3.1415926 * (2 + 7.18281828e-1) * 42\"))))\nBinaryExpr('*')\n|-- BinaryExpr('*')\n|   |-- 3.1415926\n|   `-- BinaryExpr('+')\n|       |-- 2\n|       `-- 0.718281828\n`-- 42\n</code></pre> <p>Finally, we have a proper parse tree that is easy to understand and work with!</p>"},{"location":"getting-started/parse-tree/#next","title":"Next","text":"<p>We've finished writing our numeric expressions parser.</p> <p>If you want to learn more, let's discuss a few tips and tricks about parsing in the next chapter.</p>"},{"location":"getting-started/parsing/","title":"Parsing Tokens","text":"<p>So far we have defined the tokenizer for our calculator expressions language:</p> <pre><code>&gt;&gt;&gt; from typing import List\n&gt;&gt;&gt; from funcparserlib.lexer import make_tokenizer, TokenSpec, Token\n\n\n&gt;&gt;&gt; def tokenize(s: str) -&gt; List[Token]:\n...     specs = [\n...         TokenSpec(\"whitespace\", r\"\\s+\"),\n...         TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n...         TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n...         TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n...     ]\n...     tokenizer = make_tokenizer(specs)\n...     return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n</code></pre> <p>It results a list of tokens which we want to parse according to our expressions grammar:</p> <pre><code>&gt;&gt;&gt; from pprint import pprint\n\n\n&gt;&gt;&gt; pprint(tokenize(\"3.1415926 * (2 + 7.18281828e-1) * 42\"))\n[Token('float', '3.1415926'),\n Token('op', '*'),\n Token('op', '('),\n Token('int', '2'),\n Token('op', '+'),\n Token('float', '7.18281828e-1'),\n Token('op', ')'),\n Token('op', '*'),\n Token('int', '42')]\n</code></pre>"},{"location":"getting-started/parsing/#parser-combinators","title":"Parser Combinators","text":"<p>A parser is an object that takes input tokens and transforms them into a parse result. For example, a primitive parser <code>tok(type, value)</code> parses a single token of a certain type and, optionally, with a certain value.</p> <p>Parsing a single token is not exciting at all. The interesting part comes when you start combining parsers via parser combinators to build bigger parsers of complex token sequences.</p> <p>Parsers from <code>funcparserlib.parser</code> have a nice layered structure that allows you to express the grammar rules of the langauge you want to parse:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          \u2502 Primitive Parsers    \u2502           \u2502\n\u2502          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u2502 tok(type, value)  forward_decl() \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u2502 a(token)  some(pred)  finished   \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          \u2502 Parser Combinators   \u2502           \u2502\n\u2502          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502          \u2502                                  \u2502\n\u2502  Parser  \u2502 p1 + p2   p1 | p2   p &gt;&gt; f   -p  \u2502\n\u2502  objects \u2502                                  \u2502\n\u2502          \u2502 many(p)  oneplus(p)  maybe(p)    \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502          \u2502 Means of Abstraction \u2502           \u2502\n\u2502          \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u2502 Python assignments: =            \u2502\n\u2502          \u2502                                  \u2502\n\u2502          \u2502 Python functions: def            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>You get a new <code>Parser</code> object each time you apply a parser combinator to your parsers. Therefore, the set of all parsers it closed under the operations defined by parser combinators.</p> <p>Parsers are regular Python objects of type <code>Parser</code>. It means that you can write arbitrary Python code that builds parser objects: assign parsers to variables, pass parsers as call arguments, get them as the return values of calls, etc.</p> <p>Note</p> <p>The type <code>Parser</code> is actually parameterized as <code>Parser[T, V]</code> where:</p> <ul> <li><code>T</code> is the type of input tokens</li> <li><code>V</code> is the type of the parse result</li> </ul> <p>Your text editor or type checker will provide better code completion and error checking for your parsing code based on the types defined in <code>funcparserlib</code> and their type inference capabilities.</p>"},{"location":"getting-started/parsing/#tok-parsing-a-single-token","title":"<code>tok()</code>: Parsing a Single Token","text":"<p>Let's recall the expressions we would like to be able to parse:</p> <pre><code>0\n1 + 2 + 3\n-1 + 2 ** 32\n3.1415926 * (2 + 7.18281828e-1) * 42\n</code></pre> <p>It looks like our grammar should have expressions that consist of numbers or nested expressions. Let's start with just numbers.</p> <p>We'll use <code>tok(type, value)</code> to create a primitive parser of a single integer token. Let's import it:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import tok\n</code></pre> <p>Here is our parser of a single integer token. The string <code>\"int\"</code> is the type of the integer token spec for our tokenizer:</p> <pre><code>&gt;&gt;&gt; int_str = tok(\"int\")\n</code></pre> <p>Let's try it in action. In order to invoke a parser, we should pass a sequence of tokens to its <code>Parser.parse(tokens)</code> method:</p> <pre><code>&gt;&gt;&gt; int_str.parse(tokenize(\"42\"))\n'42'\n</code></pre> <p>Note</p> <p>Our parser returns integer numbers as strings at the moment. We'll cover transforming parse results and creating a proper parse tree in the next chapter.</p> <p>If the first token in the input is not of type <code>\"int\"</code>, our parser raises an exception:</p> <pre><code>&gt;&gt;&gt; int_str.parse(tokenize(\"+\"))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '+', expected: int\n</code></pre>"},{"location":"getting-started/parsing/#p1-p2-parsing-alternatives","title":"<code>p1 | p2</code>: Parsing Alternatives","text":"<p>We want to support floating point numbers as well. We already know how to do it:</p> <pre><code>&gt;&gt;&gt; float_str = tok(\"float\")\n</code></pre> <p>Let's define our number expression as either an integer or a float number. We can parse alternatives using the <code>Parser.__or__</code> combinator:</p> <pre><code>&gt;&gt;&gt; number = int_str | float_str\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; number.parse(tokenize(\"42\"))\n'42'\n\n&gt;&gt;&gt; number.parse(tokenize(\"3.14\"))\n'3.14'\n\n&gt;&gt;&gt; number.parse(tokenize(\"*\"))  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '*', expected: int or float\n</code></pre>"},{"location":"getting-started/parsing/#p1-p2-parsing-a-sequence","title":"<code>p1 + p2</code>: Parsing a Sequence","text":"<p>Since we can parse numbers now, let's proceeed with expressions. The first expression we will parse is the power operator:</p> <pre><code>2 ** 32\n</code></pre> <p>We need a new parser combinator to parse sequences of tokens. We can combine parsers sequentially using the <code>Parser.__add__</code> combinator.</p> <p>Let's try it on sequences of numbers first:</p> <pre><code>&gt;&gt;&gt; p = number + number\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; p.parse(tokenize(\"1 2\"))\n('1', '2')\n</code></pre> <p>The sequence combinator returns its results as a tuple of the parse results of its arguments. The size of the resulting tuple depends on the number of the parsers in the sequence. Let's try it for three numbers:</p> <pre><code>&gt;&gt;&gt; p = number + number + number\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; p.parse(tokenize(\"1 2 3\"))\n('1', '2', '3')\n</code></pre> <p>Back to parsing the power operator of our calculator expressions language. We will need to parse several different operator tokens besides <code>\"**\"</code> in our grammar, so let's define a helper function:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import Parser\n\n\n&gt;&gt;&gt; def op(name: str) -&gt; Parser[Token, str]:\n...     return tok(\"op\", name)\n</code></pre> <p>Let's define the parser of the power operator expressions using our new <code>op(name)</code> helper:</p> <pre><code>&gt;&gt;&gt; power = number + op(\"**\") + number\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; power.parse(tokenize(\"2 ** 32\"))\n('2', '**', '32')\n</code></pre>"},{"location":"getting-started/parsing/#many-parsing-repeated-parts","title":"<code>many()</code>: Parsing Repeated Parts","text":"<p>We want to allow sequences of power operators. Let's parse the first number, followed by zero or more pairs of the power operator and a number. We'll use the <code>many(p)</code> combinator for that. Let's import it:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import many\n</code></pre> <p>Here is our parser of sequences of power operators:</p> <pre><code>&gt;&gt;&gt; power = number + many(op(\"**\") + number)\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; power.parse(tokenize(\"2 ** 3 ** 4\"))\n('2', [('**', '3'), ('**', '4')])\n</code></pre> <p>The <code>many(p)</code> combinator applies its argument parser <code>p</code> to the input sequence of tokens many times until it fails, returning a list of the results. If <code>p</code> fails to parse any tokens, <code>many(p)</code> still succeeds and returns an empty list:</p> <pre><code>&gt;&gt;&gt; power.parse(tokenize(\"1 + 2\"))\n('1', [])\n</code></pre>"},{"location":"getting-started/parsing/#forward_decl-parsing-recursive-parts","title":"<code>forward_decl()</code>: Parsing Recursive Parts","text":"<p>We want to allow using parentheses to specify the order of calculations.</p> <p>Ideally, we would like to write a recursive assignment like this one, but the Python syntax doesn't allow it:</p> <pre><code>&gt;&gt;&gt; expr = power | number | (op(\"(\") + expr + op(\")\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNameError: name 'expr' is not defined\n</code></pre> <p>We will use the <code>forward_decl()</code> parser to solve the recursive assignment problem:</p> <ol> <li>We create a forward declaration</li> <li>We use the declaration in other parsers</li> <li>We define the value of the declaration</li> </ol> <p>Let's start with a simple example first. We'll create a parser numbers in properly nested parentheses:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import forward_decl\n&gt;&gt;&gt; p = forward_decl()\n&gt;&gt;&gt; p.define(number | (op(\"(\") + p + op(\")\")))\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; p.parse(tokenize(\"1\"))\n'1'\n\n&gt;&gt;&gt; p.parse(tokenize(\"(1)\"))\n('(', '1', ')')\n\n&gt;&gt;&gt; p.parse(tokenize(\"((1))\"))\n('(', ('(', '1', ')'), ')')\n</code></pre> <p>Back to our recursive <code>expr</code> problem. Let's re-write our grammar using <code>forward_decl()</code> for expressions:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; parenthesized = op(\"(\") + expr + op(\")\")\n&gt;&gt;&gt; primary = number | parenthesized\n&gt;&gt;&gt; power = primary + many(op(\"**\") + primary)\n&gt;&gt;&gt; expr.define(power)\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; expr.parse(tokenize(\"2 ** 3 ** 4\"))\n('2', [('**', '3'), ('**', '4')])\n\n&gt;&gt;&gt; expr.parse(tokenize(\"2 ** (3 ** 4)\"))\n('2', [('**', ('(', ('3', [('**', '4')]), ')'))])\n</code></pre>"},{"location":"getting-started/parsing/#finished-expecting-no-more-input","title":"<code>finished</code>: Expecting No More Input","text":"<p>Surprisingly, our <code>expr</code> parser tolerates incomplete expressions by ignoring the incomplete parts:</p> <pre><code>&gt;&gt;&gt; expr.parse(tokenize(\"2 ** (3 ** 4\"))\n('2', [])\n</code></pre> <p>The problem is that its <code>many(p)</code> part parses the input while <code>p</code> succeeds, and it doesn't look any further than that. We can make a parser expect the end of the input via the <code>finished</code> parser. Let's define a parser for the whole input document:</p> <pre><code>&gt;&gt;&gt; from funcparserlib.parser import finished\n&gt;&gt;&gt; document = expr + finished\n</code></pre> <p>Note</p> <p>Usually you finish the topmost parser of your grammar with <code>... + finished</code> to indicate that you expect no further input.</p> <p>Let's try it for our grammar:</p> <pre><code>&gt;&gt;&gt; document.parse(tokenize(\"2 ** (3 ** 4\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: got unexpected end of input, expected: ')'\n\n&gt;&gt;&gt; document.parse(tokenize(\"2 ** (3 ** 4)\"))\n('2', [('**', ('(', ('3', [('**', '4')]), ')'))], None)\n</code></pre>"},{"location":"getting-started/parsing/#next","title":"Next","text":"<p>We have created a parser for power operator expressions. Its parse results are correct, but they look hard to undersand and work with:</p> <ul> <li>Our integer and floating point numbers are strings, not <code>int</code> or <code>float</code> objects</li> <li>The results contain <code>'('</code> and <code>')'</code> strings even though we need parentheses only temporarily to set the operator priorities</li> <li>The results contain <code>None</code>, which is the parse result of <code>finished</code>, even though we don't need it</li> <li>The results are lists of tuples of strings, not user-defined classes that reflect the grammar of our calculator expressions language</li> </ul> <p>In the next chapter you will learn how to transform parse results and prepare a proper, cleaned up parse tree.</p>"},{"location":"getting-started/tips-and-tricks/","title":"Tips and Tricks","text":"<p>Let's use the tokenizer we have defined previously for our examples in this chapter:</p> <pre><code>&gt;&gt;&gt; from typing import List\n&gt;&gt;&gt; from funcparserlib.lexer import make_tokenizer, TokenSpec, Token\n&gt;&gt;&gt; from funcparserlib.parser import tok, Parser, many, forward_decl, finished\n\n\n&gt;&gt;&gt; def tokenize(s: str) -&gt; List[Token]:\n...     specs = [\n...         TokenSpec(\"whitespace\", r\"\\s+\"),\n...         TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n...         TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n...         TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n...     ]\n...     tokenizer = make_tokenizer(specs)\n...     return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n\n\n&gt;&gt;&gt; def op(name: str) -&gt; Parser[Token, str]:\n...     return tok(\"op\", name)\n</code></pre>"},{"location":"getting-started/tips-and-tricks/#name-alternative-parsers-for-better-error-messages","title":"Name Alternative Parsers for Better Error Messages","text":"<p>Consider the following grammar:</p> <pre><code>&gt;&gt;&gt; number = (tok(\"int\") &gt;&gt; int) | (tok(\"float\") &gt;&gt; float)\n&gt;&gt;&gt; paren = -op(\"(\") + number + -op(\")\")\n&gt;&gt;&gt; mul = number + op(\"*\") + number\n&gt;&gt;&gt; expr = paren | mul\n</code></pre> <p>When a parser fails to parse its input, it usually reports the token it expects:</p> <pre><code>&gt;&gt;&gt; paren.parse(tokenize(\"(1\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: got unexpected end of input, expected: ')'\n</code></pre> <p>If there were several parsing alternatives, the parser will report an error after the longest successfully parsed sequence:</p> <pre><code>&gt;&gt;&gt; expr.parse(tokenize(\"1 + 2\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,3-1,3: got unexpected token: '+', expected: '*'\n</code></pre> <p>If there were several parsing alternatives and all of them failed to parse the current token, then the parser will report its name as the expected input:</p> <pre><code>&gt;&gt;&gt; number.parse(tokenize(\"*\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '*', expected: int or float\n\n&gt;&gt;&gt; expr.parse(tokenize(\"+\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '+', expected: int or float or (('(', int or float), ')')\n</code></pre> <p>Parser names are auto-generated and may be quite long and hard to understand. For better error messages you may want to name your parsers explicitly via <code>Parser.named(name)</code>. The naming style is up to you. For example:</p> <pre><code>&gt;&gt;&gt; number = ((tok(\"int\") &gt;&gt; int) | (tok(\"float\") &gt;&gt; float)).named(\"number\")\n&gt;&gt;&gt; paren = -op(\"(\") + number + -op(\")\")\n&gt;&gt;&gt; mul = number + op(\"*\") + number\n&gt;&gt;&gt; expr = (paren | mul).named(\"number or '('\")\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; number.parse(tokenize(\"*\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '*', expected: number\n\n&gt;&gt;&gt; expr.parse(tokenize(\"+\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nNoParseError: 1,1-1,1: got unexpected token: '+', expected: number or '('\n</code></pre>"},{"location":"getting-started/tips-and-tricks/#how-to-handle-conflicting-alternatives","title":"How to Handle Conflicting Alternatives","text":"<p>If one of the parsing alternatives is a subpart of another one, then you should put the longest alternative first. Otherwise parsing the shorter one will make another one unreachable:</p> <pre><code>&gt;&gt;&gt; p = (number + number) | (number + number + number)\n\n&gt;&gt;&gt; p.parse(tokenize(\"1 2 3\"))\n(1, 2)\n</code></pre> <p>Parse the longest alternative first:</p> <pre><code>&gt;&gt;&gt; p = (number + number + number) | (number + number)\n\n&gt;&gt;&gt; p.parse(tokenize(\"1 2 3\"))\n(1, 2, 3)\n\n&gt;&gt;&gt; p.parse(tokenize(\"1 2\"))\n(1, 2)\n</code></pre>"},{"location":"getting-started/tips-and-tricks/#watch-out-for-left-recursion","title":"Watch Out for Left Recursion","text":"<p>There are certain kinds grammar rules you cannot use with <code>funcparserlib</code>. These are the rules that contain recursion in their leftmost parts. These rules lead to infinite recursion during parsing, that results in a <code>RecursionError</code> exception.</p> <p>For example, we want to define an expression <code>expr</code> either a multiplication operator <code>mul</code> or a <code>number</code>. We also want an expression to be a sequence of an expression <code>expr</code>, followed by an operator <code>\"**\"</code>, followed by another expression <code>expr</code>:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; mul = expr + op(\"*\") + expr\n&gt;&gt;&gt; expr.define(mul | number)\n</code></pre> <p>This looks reasonable at the first glance, but it contains left recursion. In order to parse the first token for <code>expr</code>, we need to parse the first token for <code>mul</code>, for that we need to parse the first token for <code>expr</code>, and so on. This left recursion in your grammar results in a stack overflow exception:</p> <pre><code>&gt;&gt;&gt; expr.parse(tokenize(\"1 * 2\"))   # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\n    ...\nRecursionError: maximum recursion depth exceeded\n</code></pre> <p>You should think how to re-write your grammar to avoid left-recursive definitions. In our case of several multiplication operators we really want a number, followed by zero or more pairs of <code>*</code> and number:</p> <pre><code>&gt;&gt;&gt; expr = forward_decl()\n&gt;&gt;&gt; mul = number + many(op(\"**\") + number)\n&gt;&gt;&gt; expr.define(mul)\n</code></pre> <p>Test it:</p> <pre><code>&gt;&gt;&gt; expr.parse(tokenize(\"1 ** 2\"))\n(1, [('**', 2)])\n\n\n&gt;&gt;&gt; expr.parse(tokenize(\"3\"))\n(3, [])\n</code></pre> <p>Remember that your parsers have to consume at least one token from the input before going into recursive defintions.</p>"},{"location":"getting-started/tokenizing/","title":"Tokenizing Input","text":"<p>Parsing is usually split into two steps:</p> <ol> <li>Tokenizing the input string into a sequence of tokens</li> <li>Parsing the tokens into a parse tree</li> </ol> <pre><code>         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   str   \u2502            \u2502  List[Token]  \u2502         \u2502   Expr\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba tokenize() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba parse() \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba\n         \u2502            \u2502               \u2502         \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Tokens are larger pieces of the input text such as words, punctuation marks, spaces, etc. It's easier to parse a list of tokens than a string, since you can skip auxiliary tokens (spaces, newlines, commments) during tokenizing and focus on the main ones. Tokens usually track their position in the text, which is helpful in parsing error messages.</p>"},{"location":"getting-started/tokenizing/#tokenizing-with-make_tokenizer","title":"Tokenizing with <code>make_tokenizer()</code>","text":"<p>One of the most common ways to define tokens and tokenizing rules is via regular expressions. <code>funcparserlib</code> comes with the module <code>funcparserlib.lexer</code> for creating regexp-based tokenizers.</p> <p>Note</p> <p>Parsers defined with <code>funcparserlib</code> can work with any tokens. You can plug your custom tokenizers and token types or even parse raw strings as lists of character tokens.</p> <p>In this guide we will use the recommended way of writing tokenizers: <code>make_tokenizer()</code> from the <code>funcparserlib.lexer</code> module.</p> <p>Let's identify token types in our numeric expressions language:</p> <ul> <li>Whitespace<ul> <li>Spaces, tabs, newlines</li> </ul> </li> <li>Integer numbers<ul> <li><code>0</code>, <code>256</code>, <code>-42</code>, ...</li> </ul> </li> <li>Floating point numbers<ul> <li><code>3.1415</code>, <code>27.1828e-01</code>, ...</li> </ul> </li> <li>Operators<ul> <li><code>(</code>, <code>)</code>, <code>*</code>, <code>+</code>, <code>/</code>, <code>-</code>, <code>**</code></li> </ul> </li> </ul> <p>We will define our token specs and pass them to <code>make_tokenizer()</code> to generate our tokenizer. We will also drop whitespace tokens from the result, since we don't need them.</p> <p>Some imports first:</p> <pre><code>&gt;&gt;&gt; from typing import List\n&gt;&gt;&gt; from funcparserlib.lexer import make_tokenizer, TokenSpec, Token\n</code></pre> <p>The tokenizer itself:</p> <pre><code>&gt;&gt;&gt; def tokenize(s: str) -&gt; List[Token]:\n...     specs = [\n...         TokenSpec(\"whitespace\", r\"\\s+\"),\n...         TokenSpec(\"float\", r\"[+\\-]?\\d+\\.\\d*([Ee][+\\-]?\\d+)*\"),\n...         TokenSpec(\"int\", r\"[+\\-]?\\d+\"),\n...         TokenSpec(\"op\", r\"(\\*\\*)|[+\\-*/()]\"),\n...     ]\n...     tokenizer = make_tokenizer(specs)\n...     return [t for t in tokenizer(s) if t.type != \"whitespace\"]\n</code></pre> <p>Warning</p> <p>Be careful with ordering your token specs and your regexps so that larger tokens come first before their smaller subparts. In our token specs:</p> <ul> <li>Float tokens should come before int tokens</li> <li><code>**</code> should come before <code>*</code></li> </ul> <p>Let's try our tokenizer:</p> <pre><code>&gt;&gt;&gt; tokenize(\"42 + 1337\")\n[Token('int', '42'), Token('op', '+'), Token('int', '1337')]\n</code></pre> <p>The <code>str()</code> form of the token shows its position in the input text, also available via <code>t.start</code> and <code>t.end</code>:</p> <pre><code>&gt;&gt;&gt; [str(t) for t in tokenize(\"42 + 1337\")]\n[\"1,1-1,2: int '42'\", \"1,4-1,4: op '+'\", \"1,6-1,9: int '1337'\"]\n</code></pre>"},{"location":"getting-started/tokenizing/#next","title":"Next","text":"<p>We have tokenized an numeric expression string into a list of tokens.</p> <p>In the next chapter you will learn how to parse these tokens by defining a grammar for our numeric expressions language.</p>"}]}